{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VglQx7QcVUMW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT6KiMobVWqn"
      },
      "outputs": [],
      "source": [
        "# !pip install scikit-learn==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEZSyH8AVYuM",
        "outputId": "e5c3f2b8-f216-43d6-8acc-e1110a683c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.2\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNZ5vGJrVeW5"
      },
      "outputs": [],
      "source": [
        "# Define variables\n",
        "model = \"RCL\"\n",
        "folder = f'{model}/'\n",
        "output_folder = \"output_files/\"\n",
        "\n",
        "# Function to read and filter CSV files from a folder based on a timestamp threshold\n",
        "def read_and_filter_csv(folder_path, timestamp_threshold):\n",
        "    dataframes = []\n",
        "\n",
        "    # Iterate over files in the specified folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(folder_path, filename)  # Construct the full file path\n",
        "            df = pd.read_csv(file_path)  # Read the CSV file into a pandas DataFrame\n",
        "            df = df[df['Timestamp'] >= timestamp_threshold]  # Filter rows based on timestamp threshold\n",
        "            df['Source'] = filename  # Add a new column to indicate the source file\n",
        "            dataframes.append(df)  # Append the filtered DataFrame to the list\n",
        "\n",
        "    return dataframes  # Return a list of filtered DataFrames\n",
        "\n",
        "# Read and filter all CSV files in the specified folder (adjust '7' as needed for timestamp threshold)\n",
        "data = read_and_filter_csv(folder, 7)\n",
        "\n",
        "# Concatenate all filtered DataFrames into a single DataFrame\n",
        "train = pd.concat(objs=data[:], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1edy_9OdV9Sf"
      },
      "outputs": [],
      "source": [
        "# Extracting targets\n",
        "targets_numpy = train[\"Label\"]\n",
        "\n",
        "# Extracting features\n",
        "features_numpy = train.iloc[:, 1:-1]\n",
        "\n",
        "# Selecting specific features\n",
        "features_numpy = features_numpy[[\"Data1\", \"Data2\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EYbfFzvWG1B"
      },
      "outputs": [],
      "source": [
        "def create_Windowed_Dataset(Time, Dataset, Labelset, windowsize, stepsize):\n",
        "    # Initialize empty lists and variables\n",
        "    end = Dataset.shape[0]\n",
        "    num_columns = Dataset.shape[1]\n",
        "    New_Dataset = []\n",
        "    New_Labelset = []\n",
        "    from_timestamps = []\n",
        "    to_timestamps = []\n",
        "\n",
        "    # Iterate through the Dataset with a sliding window\n",
        "    for i in range(windowsize, end, stepsize):\n",
        "        # Extract a window of data from Dataset and convert to a flattened numpy array\n",
        "        row = Dataset.iloc[i-windowsize:i, :].to_numpy().flatten()\n",
        "\n",
        "        # Record timestamps for the window\n",
        "        from_timestamps.append(Time.iloc[i-windowsize])\n",
        "        to_timestamps.append(Time.iloc[i])\n",
        "\n",
        "        # Determine label based on model type ('blink' or otherwise)\n",
        "        if model == \"blink\":\n",
        "            if Labelset.iloc[i] == Labelset.iloc[i-8]:\n",
        "                label = Labelset.iloc[i-8]\n",
        "            else:\n",
        "                label = 0\n",
        "        else:\n",
        "            if Labelset.iloc[i] == Labelset.iloc[i-8]:\n",
        "                label = Labelset.iloc[i]\n",
        "            else:\n",
        "                label = 1\n",
        "\n",
        "        # Check for NaN or Inf values in the window; skip if found\n",
        "        if np.any(np.isinf(row)) or np.any(np.isnan(row)):\n",
        "            continue\n",
        "\n",
        "        # Append data to respective lists\n",
        "        New_Dataset.append(row)\n",
        "        New_Labelset.append(label)\n",
        "    # Convert lists to numpy arrays\n",
        "    New_Dataset = np.array(New_Dataset)\n",
        "    New_Labelset = np.array(New_Labelset)\n",
        "\n",
        "    # Return the processed data\n",
        "    return New_Dataset, New_Labelset, from_timestamps, to_timestamps\n",
        "\n",
        "New_dataset, New_Labelset, from_timestamps, to_timestamps= create_Windowed_Dataset(train['Timestamp'], features_numpy, targets_numpy, 50, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCboUKHutQzx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOUBHG5yWJT5",
        "outputId": "6c80869e-2d51-40d4-a2cc-6b46804732f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "1        73160\n",
              "0         3758\n",
              "2         3589\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Convert New_dataset to a pandas DataFrame\n",
        "New_dataset = pd.DataFrame(New_dataset)\n",
        "\n",
        "# Convert New_Labelset to a pandas DataFrame with column name 'Label'\n",
        "New_Labelset = pd.DataFrame(New_Labelset, columns=['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6E2324YWQJm"
      },
      "outputs": [],
      "source": [
        "# !pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWkebIfeWShm",
        "outputId": "4ec5c6f4-e16f-4c0e-ddf2-849548d5bed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original dataset shape:\n",
            " Label\n",
            "1        73160\n",
            "0         3758\n",
            "2         3589\n",
            "Name: count, dtype: int64\n",
            "Resample dataset shape\n",
            " Label\n",
            "0        73160\n",
            "1        73160\n",
            "2        73160\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries from imblearn for resampling techniques\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Initialize RandomOverSampler with a random state for reproducibility\n",
        "rus = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Resample the dataset using RandomOverSampler\n",
        "x_rus, y_rus = rus.fit_resample(New_dataset, New_Labelset)\n",
        "\n",
        "# Print original and resampled dataset shapes\n",
        "print('Original dataset shape:\\n', New_Labelset.value_counts())\n",
        "print('Resampled dataset shape:\\n', y_rus.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWaNjqxDWnhZ",
        "outputId": "a86a740a-5f44-4f46-8736-5e30913804a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-1f94585df65c>:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  classifier_rf.fit(x_rus, y_rus)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Accuracy: 0.9959221796974668\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier_rf = RandomForestClassifier(random_state=42, n_jobs=-1,n_estimators=100, oob_score=True)\n",
        "# classifier_rf = RandomForestClassifier(bootstrap=True,max_depth=None,max_features='log2',min_samples_leaf=1,min_samples_split=2,random_state=42, n_jobs=-1,n_estimators=246, oob_score=True)\n",
        "\n",
        "classifier_rf.fit(x_rus, y_rus)\n",
        "print(f'OOB Accuracy: {classifier_rf.oob_score_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3hoCEOs4dg-"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# from scipy.stats import randint\n",
        "\n",
        "# param_dist = {\n",
        "#     'n_estimators': randint(100, 300),\n",
        "#     'max_features': ['auto', 'log2'],\n",
        "#     'max_depth': [None, 10, 20, 30],\n",
        "#     'min_samples_split': randint(2, 10),\n",
        "#     'min_samples_leaf': randint(1, 4),\n",
        "#     'bootstrap': [True, False]\n",
        "# }\n",
        "\n",
        "# random_search = RandomizedSearchCV(estimator=classifier_rf, param_distributions=param_dist, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
        "# random_search.fit(x_rus, y_rus)\n",
        "\n",
        "# best_params = random_search.best_params_\n",
        "# print(best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5WotaEnWoEt",
        "outputId": "fcba621c-f006-4852-c7b0-ad842e59a4c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RCL_30.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "filename = f'{model}.joblib'\n",
        "# save model\n",
        "joblib.dump(classifier_rf, filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVz0ckJ84g9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}